{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal # Requires Python 3.8+\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from parcellation_surface.visualization import visualize_brain_surface\n",
    "from parcellation_surface.smoothing import smooth_surface_graph\n",
    "from parcellation_surface.watershed import watershed_by_flooding\n",
    "from parcellation_surface.gradient import build_mesh_graph\n",
    "from parcellation_surface.group_analysis import create_random_parcels, homogeneity_craddock_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fmri_timeseries(dataset = Literal[\"PPSFACE_N18\", \"PPSFACE_N20\"],\n",
    "                            hemisphere = Literal[\"lh\", \"rh\"],\n",
    "                            run = Literal[\"1\",\"2\"]):\n",
    "    # Load all of the surf fmri data\n",
    "    surf_fmri_list = []\n",
    "    dataset_dir = f\"D:\\Data_Conn_Preproc\\\\{dataset}\"\n",
    "    if dataset == \"PPSFACE_N18\":\n",
    "        n = 19\n",
    "    else:\n",
    "        n = 21\n",
    "    for i in range(1,n):\n",
    "        if i == 5 and dataset==\"PPSFACE_N20\": # Subject 5 is missing\n",
    "            continue\n",
    "        subject = f\"{i:02d}\"\n",
    "        subj_dir = dataset_dir + r\"\\sub-\" + subject\n",
    "        fmri_path = subj_dir + f\"\\\\func\\surf_conn_sub{subject}_run{run}_{hemisphere}.func.fsaverage6.mgh\"\n",
    "        surf_fmri_img = nib.load(fmri_path)\n",
    "        surf_fmri = surf_fmri_img.get_fdata()\n",
    "        surf_fmri = np.squeeze(surf_fmri) # just rearrange the MGH data\n",
    "        # Normilize the data\n",
    "        surf_fmri = (surf_fmri - np.mean(surf_fmri, axis=1, keepdims=True)) / np.std(surf_fmri, axis=1, keepdims=True)\n",
    "        # Replace nan values with 0\n",
    "        surf_fmri = np.nan_to_num(surf_fmri)\n",
    "        surf_fmri_list.append(surf_fmri)\n",
    "    return surf_fmri_list\n",
    "\n",
    "def extract_parcellation(dataset = Literal[\"PPSFACE_N18\", \"PPSFACE_N20\"], \n",
    "                         hemi = Literal[\"lh\", \"rh\"], \n",
    "                         run = Literal[\"1\",\"2\"]):\n",
    "    # Load the parcellation\n",
    "    # bound_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N20\\\\boundary_smoothed_map_allsub_run{run}_{hemi}.npy\"\n",
    "    # grad_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N20\\gradients_smoothed_allsub_run{run}_{hemi}.npy\"\n",
    "    bound_path = f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\"\n",
    "    grad_path = f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\gradients_smoothed_group_run{run}_{hemi}.npy\"\n",
    "    bound = np.load(bound_path)\n",
    "    grad = np.load(grad_path)\n",
    "    return bound, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the parcellations\n",
    "# For ppsface n18\n",
    "ppsface = \"PPSFACE_N20\"\n",
    "for hemi in [\"lh\", \"rh\"]:\n",
    "   surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "   inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "   # surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "   coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "   coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "   graph = build_mesh_graph(faces)\n",
    "   bound_sum = 0\n",
    "   for run in [\"1\",\"2\"]:\n",
    "      # Extract the parcellation\n",
    "      bound_sum += np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\")\n",
    "        \n",
    "   # Create the parcels\n",
    "   bound_smoothed = smooth_surface_graph(graph, bound_sum, iterations=10)\n",
    "   label_bound = watershed_by_flooding(graph, bound_smoothed)\n",
    "   # Save the parcels\n",
    "   np.save(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_{hemi}.npy\", label_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'lh'\n",
    "label = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_{hemi}.npy\")\n",
    "print((np.unique(label)>=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a parcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_lh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.inflated\"\n",
    "pial_lh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.white\"\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(inf_lh_path)\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(inf_lh_path)\n",
    "graph = build_mesh_graph(faces_lh)\n",
    "label = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_lh.npy\")\n",
    "# label = create_random_parcels(graph, 240)\n",
    "visualize_brain_surface(coords_lh, faces_lh, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show both hemisphere boundary maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import combine_surfaces\n",
    "# Load the parcellation for lh run1\n",
    "run = \"1\"\n",
    "dataset = \"PPSFACE_N18\"\n",
    "white_lh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.white\"\n",
    "white_rh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\\\rh.white\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(white_lh_path)\n",
    "coords_rh, faces_rh = nib.freesurfer.read_geometry(white_rh_path)\n",
    "\n",
    "# Extract bounardy and grad maps\n",
    "bound_lh, _ = extract_parcellation(dataset, 'lh', run)\n",
    "bound_rh, _ = extract_parcellation(dataset, 'rh', run)\n",
    "\n",
    "# Combine surface meshes\n",
    "coords_c, faces_c, scalar_c = combine_surfaces(coords_lh, faces_lh, bound_lh,\n",
    "                                              coords_rh, faces_rh, bound_rh)\n",
    "\n",
    "visualize_brain_surface(coords_c, faces_c, scalar_c, title='Boundary map ', cmap='cold_hot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import combine_surfaces\n",
    "import numpy as np\n",
    "\n",
    "run = \"1\"\n",
    "dataset = \"PPSFACE_N18\"\n",
    "inf_lh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.inflated\"\n",
    "inf_rh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\\\rh.inflated\"\n",
    "# 1. Read coordinates and faces as you do now\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(inf_lh_path)\n",
    "coords_rh, faces_rh = nib.freesurfer.read_geometry(inf_rh_path)\n",
    "# Extract bounardy and grad maps\n",
    "bound_lh, _ = extract_parcellation(dataset, 'lh', run)\n",
    "bound_rh, _ = extract_parcellation(dataset, 'rh', run)\n",
    "\n",
    "# 2. Compute the bounding box (or max extent) for the left hemisphere\n",
    "#    so we know how far to shift the right hemisphere.\n",
    "lh_max_x = np.max(coords_lh[:, 0])\n",
    "lh_min_x = np.min(coords_lh[:, 0])\n",
    "lh_width = lh_max_x - lh_min_x\n",
    "\n",
    "# 3. Shift the right hemisphere by at least that width + a small margin\n",
    "coords_rh[:, 0] += lh_width + 4  # 5 mm margin, for instance\n",
    "\n",
    "# Now the two hemispheres won't overlap\n",
    "coords_c, faces_c, scalar_c = combine_surfaces(coords_lh, faces_lh, bound_lh,\n",
    "                                               coords_rh, faces_rh, bound_rh)\n",
    "visualize_brain_surface(coords_c, faces_c, scalar_c, title='Boundary map', cmap='cold_hot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Coef comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice coefficient Run1 vs Run2\n",
    "from group_analysis import dice_coefficient\n",
    "dataset = \"PPSFACE_N20\"\n",
    "\n",
    "hemi = \"lh\"\n",
    "run = \"1\"\n",
    "\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "graph = build_mesh_graph(faces)\n",
    "\n",
    "\n",
    "bound = np.load(f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\")\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "label_bound_run1 = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "\n",
    "run = \"2\"\n",
    "bound = np.load(f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\")\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "label_bound_run2 = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "dice_p = dice_coefficient((label_bound_run1>=0)*1, (label_bound_run2>=0)*1)\n",
    "# dice_b = dice_coefficient((label_bound_run1<0)*1, (label_bound_run2<0)*1)\n",
    "\n",
    "print(f\"Number of parcels run1: {(np.unique(label_bound_run1)>=0).sum()}\")\n",
    "print(f\"Number of parcels run2: {(np.unique(label_bound_run2)>=0).sum()}\")\n",
    "\n",
    "\n",
    "print(f\"Run 1-2 Dice coefficient on parcels : {dice_p}\")\n",
    "# print(f\"Run 1-2 Dice coefficient on boundaries: {dice_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice coefficient PPSFACE_N18 vs PPSFACE_N20\n",
    "dataset1 = \"PPSFACE_N18\"\n",
    "dataset2 = \"PPSFACE_N20\"\n",
    "\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\group_parcellation\\labels_group_run12_smooth10_lh.npy\"\n",
    "hemi = \"lh\"\n",
    "parcel1 = np.load(f\"D:\\Data_Conn_Preproc\\{dataset1}\\group_parcellation\\\\labels_group_run12_smooth10_{hemi}.npy\")\n",
    "parcel2 = np.load(f\"D:\\Data_Conn_Preproc\\{dataset2}\\group_parcellation\\\\labels_group_run12_smooth10_{hemi}.npy\")\n",
    "dice_p = dice_coefficient((parcel1>=0)*1, (parcel2>=0)*1)\n",
    "\n",
    "print(f\"Number of parcels dataset1: {(np.unique(parcel1)>=0).sum()}\")\n",
    "print(f\"Number of parcels dataset2: {(np.unique(parcel2)>=0).sum()}\")\n",
    "print(f\"dataset 1-2 Dice coefficient on parcels : {dice_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice_rnd = []\n",
    "dice_rnd_parc = []\n",
    "for i in range(100):\n",
    "    # Compare the Dice indice for random parcels of same size\n",
    "    rnd_parc_1 = create_random_parcels(graph,n_clusters=(np.unique(label_bound_run1)>=0).sum())\n",
    "    rnd_parc_2 = create_random_parcels(graph,n_clusters=(np.unique(label_bound_run2)>=0).sum())\n",
    "\n",
    "    dice_p_ = dice_coefficient((rnd_parc_1>=0)*1, (rnd_parc_2>=0)*1)\n",
    "    # dice_b_ = dice_coefficient((rnd_parc_1<0)*1, (rnd_parc_2<0)*1)\n",
    "    dice_rnd_parc.append(dice_p_)\n",
    "    # dice_rnd.append(dice_b_)\n",
    "    \n",
    "    # print(f\"Random 1-2 Dice coefficient on parcels: {dice_b_}\")\n",
    "    # print(f\"Random 1-2 Dice coefficient on parcels: {dice_p_}\")\n",
    "print(f\"Random 1-2 Dice coefficient on parcels: mean {np.mean(dice_rnd_parc)} std {np.std(dice_rnd_parc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_pps18run1vsrun2 = 0.7728\n",
    "rh_pps20run1vsrun2 = 0.7731\n",
    "rh_pps18vspps20 = 0.7817\n",
    "\n",
    "lh_pps18run1vsrun2 = 0.7716\n",
    "lh_pps20run1vsrun2 = 0.7803\n",
    "lh_pps18vspps20 = 0.7743\n",
    "\n",
    "\n",
    "# plot the homogenity scores\n",
    "x_null = np.ones(len(dice_rnd_parc)) + np.random.uniform(-0.05, 0.05, len(dice_rnd_parc))\n",
    "x_model = np.array([1]) \n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "offsets = [-0.02, 0, 0.02]\n",
    "\n",
    "plt.scatter(x_model + offsets[0], [lh_pps18run1vsrun2], \n",
    "            color='c', marker='<', s=100, label='Run1 vs Run2 PPSFACE18')\n",
    "plt.scatter(x_model + offsets[1], [lh_pps20run1vsrun2], \n",
    "            color='c', marker='>', s=100, label='Run1 vs Run2 PPSFACE20')\n",
    "plt.scatter(x_model + offsets[2], [lh_pps18vspps20], \n",
    "            color='c', marker='s', s=100, label='PPSFACE20 vs PPSFACE18')\n",
    "\n",
    "plt.scatter(x_null, dice_rnd_parc, color='black', alpha=0.7, label='Null Model, 100 trials')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([1],'')  # Single x-tick since both are on the same column\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.title('Consistency of Parcellations in Left Hemisphere')\n",
    "plt.legend(\n",
    "    loc='lower center',            \n",
    "    bbox_to_anchor=(0.5, -0.15),   \n",
    "    fancybox=True, shadow=True,    \n",
    "    ncol=2                         \n",
    ")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an homogenity metric like craddock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parcellation for lh run1\n",
    "dataset = \"PPSFACE_N20\"\n",
    "hemi = \"lh\"\n",
    "# run = \"1\"\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "graph = build_mesh_graph(faces)\n",
    "\n",
    "# Extract parcellation and grad maps\n",
    "label = np.load(f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\\\labels_group_run12_smooth10_{hemi}.npy\")\n",
    "# bound = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\")\n",
    "# bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "# label_bound = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "# Load all of the surf fmri data form run 1 lh\n",
    "surf_fmri_list1 = extract_fmri_timeseries(dataset=dataset, hemisphere=hemi, run='1')\n",
    "surf_fmri_list2 = extract_fmri_timeseries(dataset=dataset, hemisphere=hemi, run='2')\n",
    "surf_fmri_list = surf_fmri_list1 + surf_fmri_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset, '   ')\n",
    "print('mean and var', surf_fmri_list[2].mean(), surf_fmri_list[2].var())\n",
    "print('list length', len(surf_fmri_list))\n",
    "print('number of parcels', (np.unique(label)>=0).sum())\n",
    "\n",
    "homogeneity_model = homogeneity_craddock_rt(label, surf_fmri_list)\n",
    "print(f\"Model homogeneity: {homogeneity_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the null model on the\n",
    "homogenity_null_list = []\n",
    "for n_sample in range(1, 1000):\n",
    "    label_null = create_random_parcels(graph, n_clusters=(np.unique(label)>=0).sum()) # Create a null model\n",
    "    homogeneity_null = homogeneity_craddock_rt(label_null, surf_fmri_list)\n",
    "    homogenity_null_list.append(homogeneity_null)\n",
    "    print(f\"Null homogeneity: {homogeneity_null}\")\n",
    "\n",
    "np.save(f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\{dataset}_homogeneity_{hemi}_1000.npy\", \n",
    "        homogenity_null_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogenity_null_list = np.load(f\"D:\\Data_Conn_Preproc\\{dataset}\\group_parcellation\\{dataset}_homogeneity_{hemi}_1000.npy\")\n",
    "print(np.mean(homogenity_null_list), np.std(homogenity_null_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the homogenity scores\n",
    "x_null = np.ones(len(homogenity_null_list)) + np.random.uniform(-0.05, 0.05, len(homogenity_null_list))\n",
    "x_model = np.array([1]) \n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(x_model, [homogeneity_model], color='red', s=100, label='My Model')\n",
    "plt.scatter(x_null, homogenity_null_list, color='black', alpha=0.7, label='Null Model')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([1],'')  # Single x-tick since both are on the same column\n",
    "plt.ylabel('Homogeneity')\n",
    "plt.title('Comparison of Homogeneity (PPSFACE20)')\n",
    "plt.legend(\n",
    "    loc='lower center',            \n",
    "    bbox_to_anchor=(0.5, -0.15),   \n",
    "    fancybox=True, shadow=True,    \n",
    "    ncol=1                         \n",
    ")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the subject level parcellation better for homogenity than group parcellation ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parcellation for lh run1\n",
    "hemi = \"lh\"\n",
    "run = \"1\"\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "graph = build_mesh_graph(faces)\n",
    "\n",
    "# Extract bounardy and grad maps\n",
    "bound, grad = extract_parcellation(hemi, run)\n",
    "\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "grad_smoothed = smooth_surface_graph(graph, np.mean(grad,axis=1), iterations=10)\n",
    "\n",
    "label_grad = watershed_by_flooding(graph, grad_smoothed)\n",
    "label_bound = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "# Load all of the surf fmri data form run 1 lh\n",
    "surf_fmri_list = extract_fmri_timeseries(hemi, run)\n",
    "\n",
    "# subject 4 parcellation and data\n",
    "bound_sub4 = np.load(r\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\sub-04\\sub-04_parcellation\\boundary_map_sub04_run1_lh.npy\")\n",
    "surf_fmri_s4 = [surf_fmri_list[3]]\n",
    "bound_smoothed_sub4 = smooth_surface_graph(graph, bound_sub4, iterations=10)\n",
    "label_sub4 = watershed_by_flooding(graph, bound_smoothed_sub4)\n",
    "\n",
    "\n",
    "bound_sub3 = np.load(r\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\sub-03\\sub-03_parcellation\\boundary_map_sub03_run1_lh.npy\")\n",
    "surf_fmri_s3 = [surf_fmri_list[2]]\n",
    "bound_smoothed_sub3 = smooth_surface_graph(graph, bound_sub3, iterations=10)\n",
    "label_sub3 = watershed_by_flooding(graph, bound_smoothed_sub3)\n",
    "\n",
    "\n",
    "bound_sub2 = np.load(r\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\sub-02\\sub-02_parcellation\\boundary_map_sub02_run1_lh.npy\")\n",
    "surf_fmri_s2 = [surf_fmri_list[1]]\n",
    "bound_smoothed_sub2 = smooth_surface_graph(graph, bound_sub2, iterations=10)\n",
    "label_sub2 = watershed_by_flooding(graph, bound_smoothed_sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the group parcellation homogenity on subject 4 fmri \n",
    "homogeneity_group4 = homogeneity_craddock_rt(label_bound, surf_fmri_s4)\n",
    "homogeneity_n4 = homogeneity_craddock_rt(label_sub4, surf_fmri_s4)\n",
    "\n",
    "print(f\"Group homogeneity: {homogeneity_group4}\")\n",
    "print(f\"Subject 4 homogeneity: {homogeneity_n4}\") # IMPORTANT RESULTS HERE !!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the group parcellation homogenity on subject 4 fmri \n",
    "homogeneity_group3 = homogeneity_craddock_rt(label_bound, surf_fmri_s3)\n",
    "homogeneity_n3 = homogeneity_craddock_rt(label_sub3, surf_fmri_s3)\n",
    "\n",
    "print(f\"Group homogeneity: {homogeneity_group3}\")\n",
    "print(f\"Subject 3 homogeneity: {homogeneity_n3}\") # IMPORTANT RESULTS HERE !!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the group parcellation homogenity on subject 4 fmri \n",
    "homogeneity_group2 = homogeneity_craddock_rt(label_bound, surf_fmri_s2)\n",
    "homogeneity_n2 = homogeneity_craddock_rt(label_sub2, surf_fmri_s2)\n",
    "\n",
    "print(f\"Group homogeneity: {homogeneity_group2}\")\n",
    "print(f\"Subject 3 homogeneity: {homogeneity_n2}\") # IMPORTANT RESULTS HERE !!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace these with actual values)\n",
    "list_sublvl = [homogeneity_n2, homogeneity_n3, homogeneity_n4]\n",
    "list_group = [homogeneity_group2, homogeneity_group3, homogeneity_group4]\n",
    "labels = [\"Parcellation sub-1\", \"Parcellation sub-2\", \"Parcellation sub-3\"]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "index = np.arange(len(labels))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(index, list_sublvl, marker='o', linestyle='-', color='b', label=\"Subject-Level Parcellation\")\n",
    "plt.plot(index, list_group, marker='s', linestyle='-', color='r', label=\"Group-Level Parcellation\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.ylabel(\"Homogeneity Score\")\n",
    "plt.title(\"Comparison of Subject-Level vs Group-Level Parcellation\")\n",
    "plt.xticks(index, labels)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph theory analysis\n",
    "\n",
    "## 1 Metric Part\n",
    "In this part we will anaylse the parcellation by amking it into a graph. \n",
    "\n",
    "For that, we average the time course in the parcels to have an average parcel time course\n",
    "\n",
    "And then we make a correaltion of these time courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nibabel as nib\n",
    "from parcellation_surface.preprocessing_surface import extract_fmri_timeseries\n",
    "from parcellation_surface.gradient import build_mesh_graph\n",
    "from parcellation_surface.group_analysis import create_random_parcels\n",
    "\n",
    "# Make it for PPSFACE18 on run 1 and run 2\n",
    "ppsface = \"PPSFACE_N20\"\n",
    "\n",
    "surface_path_lh = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.white\"\n",
    "surface_path_rh = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\\\rh.white\"\n",
    "_, faces_lh = nib.freesurfer.read_geometry(surface_path_lh)\n",
    "_, faces_rh = nib.freesurfer.read_geometry(surface_path_rh)\n",
    "\n",
    "# Load the parcellation for lh run1\n",
    "label_lh = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_lh.npy\")\n",
    "label_rh = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_rh.npy\")\n",
    "label_rh[label_rh>=0] += np.max(label_lh) + 1 # 40k x 1, new values for the labels\n",
    "# Load all of the surf fmri data form run 1 lh\n",
    "surf_fmri_list_1_lh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'lh', run = '1')\n",
    "surf_fmri_list_2_lh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'lh', run = '2')\n",
    "surf_fmri_list_1_rh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'rh', run = '1')\n",
    "surf_fmri_list_2_rh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'rh', run = '2')\n",
    "\n",
    "# With a random parcellation\n",
    "graph_lh = build_mesh_graph(faces_lh)\n",
    "graph_rh = build_mesh_graph(faces_rh)\n",
    "label_lh_rnd = create_random_parcels(graph_lh, (np.unique(label_lh)>=0).sum())\n",
    "label_rh_rnd = create_random_parcels(graph_rh, (np.unique(label_rh)>=0).sum())\n",
    "label_rh_rnd[label_rh_rnd>=0] += np.max(label_lh_rnd) + 1 # 40k x 1, new values for the labels (negatives val dont move)\n",
    "\n",
    "\n",
    "label_rh = label_rh_rnd\n",
    "label_lh = label_lh_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT ALL TOGETHER\n",
    "from parcellation_surface.group_analysis import parcel_correlation\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "# Metrix used for the graph theory part\n",
    "clustering_coef_list = []\n",
    "modularity_list = []\n",
    "average_shortest_path_list = []\n",
    "small_world_list = [] # sigma values\n",
    "\n",
    "\n",
    "for subject in range(len(surf_fmri_list_1_lh)):\n",
    "    print(f\"Subject {subject}\")\n",
    "    # Get the parcellation correlatino for lh and rh\n",
    "    # Group de run 1 and 2\n",
    "    run1_lh = surf_fmri_list_1_lh[subject]\n",
    "    run2_lh = surf_fmri_list_2_lh[subject]\n",
    "    run12_lh = np.concatenate((run1_lh, run2_lh), axis=1) # 40k x 760\n",
    "    run1_rh = surf_fmri_list_1_rh[subject]\n",
    "    run2_rh = surf_fmri_list_2_rh[subject]\n",
    "    run12_rh = np.concatenate((run1_rh, run2_rh), axis=1) # 40k x 760\n",
    "    \n",
    "    label_both_hemi = np.concatenate((label_lh, label_rh), axis=0) # 80k x 1\n",
    "    \n",
    "    run12 = np.concatenate((run12_lh, run12_rh), axis=0) # 80k x 760\n",
    "    \n",
    "    \n",
    "    # Create the correlation matrix\n",
    "    parcel_corr = parcel_correlation(label_both_hemi, run12)\n",
    "    \n",
    "    # Get the 15% percentile of most correlate values\n",
    "    min_corr = np.percentile(parcel_corr, 80)\n",
    "    parcel_corr[parcel_corr<min_corr] = 0\n",
    "    # \n",
    "    distance = (parcel_corr>0)*1\n",
    "    link_strength = distance\n",
    "    # distance = 1-parcel_corr\n",
    "    # link_strength = parcel_corr + 1\n",
    "    assert np.allclose(distance, distance.T), \"correlation matrix is not symmetric\"\n",
    "\n",
    "    # Create a grpah for clustering coef  shortest path length\n",
    "    G = nx.from_numpy_array(link_strength)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) # remove self loops (diagonal)\n",
    "    # Create a graph for shortest path length\n",
    "    G_spl = nx.from_numpy_array(distance)\n",
    "    G_spl.remove_edges_from(nx.selfloop_edges(G_spl)) # remove self loops (diagonal)\n",
    "    # create a random unidirected graph with 15% density\n",
    "    G = nx.erdos_renyi_graph((np.unique(label_both_hemi)>=0).sum(), 0.20) # used for comparison\n",
    "    G_spl = nx.erdos_renyi_graph((np.unique(label_both_hemi)>=0).sum(), 0.20) # used for comparison\n",
    "    \n",
    "    \n",
    "    # The clustering coef\n",
    "    avg_clustering = nx.average_clustering(G, weight='weight')\n",
    "    clustering_coef_list.append(avg_clustering)\n",
    "    \n",
    "    # The modularity\n",
    "    communities = list(greedy_modularity_communities(G, weight='weight'))\n",
    "    mod_value = modularity(G, communities, weight='weight')\n",
    "    modularity_list.append(mod_value)\n",
    "    \n",
    "    # Avergae shortest path length\n",
    "    if nx.is_connected(G_spl):\n",
    "        avg_shortest_path = nx.average_shortest_path_length(G_spl, weight='weight')\n",
    "    else:\n",
    "        print('Not connected')\n",
    "        components = list(nx.connected_components(G_spl))\n",
    "        avg_lengths = [nx.average_shortest_path_length(G_spl.subgraph(comp), weight='weight')\n",
    "                    for comp in components if len(comp) > 1]\n",
    "        avg_shortest_path = np.mean(avg_lengths) if avg_lengths else None\n",
    "    average_shortest_path_list.append(avg_shortest_path)\n",
    "    \n",
    "    # (Optional) Small-World Analysis.\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    G_rand = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    rand_clustering = nx.average_clustering(G_rand)\n",
    "    if nx.is_connected(G_rand):\n",
    "        rand_shortest_path = nx.average_shortest_path_length(G_rand)\n",
    "    else:\n",
    "        comp_rand = list(nx.connected_components(G_rand))\n",
    "        rand_lengths = [nx.average_shortest_path_length(G_rand.subgraph(comp))\n",
    "                        for comp in comp_rand if len(comp) > 1]\n",
    "        rand_shortest_path = np.mean(rand_lengths) if rand_lengths else None\n",
    "\n",
    "    if rand_shortest_path is not None and avg_shortest_path is not None:\n",
    "        sigma = (avg_clustering / rand_clustering) / (avg_shortest_path / rand_shortest_path)\n",
    "    else:\n",
    "        sigma = None\n",
    "    small_world_list.append(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the mean and std of each list \n",
    "print('for rnd graph on ppsface20, no weight 20percent connexions')\n",
    "print(f\"Mean clustering coef: {np.mean(clustering_coef_list)}\", f\"Std clustering coef: {np.std(clustering_coef_list)}\")\n",
    "print(f\"Mean modularity: {np.mean(modularity_list)}\", f\"Std modularity: {np.std(modularity_list)}\")\n",
    "print(f\"Mean average shortest path: {np.mean(average_shortest_path_list)}\", f\"Std average shortest path: {np.std(average_shortest_path_list)}\")\n",
    "print(f\"Mean small world: {np.mean(small_world_list)}\", f\"Std small world: {np.std(small_world_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1-coor for distance, 1+corr for edge weight ---> keeping all of the links ---> bad idea\n",
    "The distance is not valuable like that... the less percent of correlation we keep the best is the small worldness.\n",
    "\n",
    "With all of the links : \n",
    "\n",
    "- Mean clustering coef: 0.5362756242701668 Std clustering coef: 0.014066995289751213\n",
    "- Mean modularity: 0.04574750578995522 Std modularity: 0.0123708154468624\n",
    "- Mean average shortest path: 0.9260425933152088 Std average shortest path: 0.02371690189496446\n",
    "- Mean small world: 0.5798353331931781 Std small world: 0.02963548039694061\n",
    "\n",
    "\n",
    "Random parcellation :\n",
    "\n",
    "- Mean clustering coef: 0.268500971073205 Std clustering coef: 0.030815816609678116\n",
    "- Mean modularity: 0.3858715669299432 Std modularity: 0.03755719653882077\n",
    "- Mean average shortest path: 0.9075367719586135 Std average shortest path: 0.01660680582718532\n",
    "- Mean small world: 3.712423345046655 Std small world: 0.487593423875435\n",
    "\n",
    "\n",
    "for random parels on ppsface20\n",
    "- Mean clustering coef: 0.27547801169118535 Std clustering coef: 0.03228586551964928\n",
    "- Mean modularity: 0.38871774970003786 Std modularity: 0.039183588602628994\n",
    "- Mean average shortest path: 0.9101651841808298 Std average shortest path: 0.014618571364787772\n",
    "- Mean small world: 3.7895583358363147 Std small world: 0.5110324977906517\n",
    "\n",
    "for  parels on ppsface20\n",
    "- Mean clustering coef: 0.26451002929199907 Std clustering coef: 0.0342991075758053\n",
    "- Mean modularity: 0.3996314985892854 Std modularity: 0.04169672463991428\n",
    "- Mean average shortest path: 0.9147347562112222 Std average shortest path: 0.013401760386203608\n",
    "- Mean small world: 3.618138570579604 Std small world: 0.5168140393944595\n",
    "\n",
    "for  parels rnd on ppsface18, no weight 20percent connexions \n",
    "- Mean clustering coef: 0.5534898545587127 Std clustering coef: 0.02973890598361682\n",
    "- Mean modularity: 0.3153656387982766 Std modularity: 0.036957117704235806\n",
    "- Mean average shortest path: 1.9831311740597803 Std average shortest path: 0.06565853374026975\n",
    "- Mean small world: 2.5347296782173587 Std small world: 0.06626107458262778\n",
    "\n",
    "for  parels rnd on ppsface20, on weight 20percent connexions\n",
    "Mean clustering coef: 0.5631306961800053 Std clustering coef: 0.03133179680242779\n",
    "Mean modularity: 0.32931525373905596 Std modularity: 0.02867038544013402\n",
    "Mean average shortest path: 1.959159886152849 Std average shortest path: 0.09042579496997746\n",
    "Mean small world: 2.618559236539185 Std small world: 0.2238256408488425\n",
    "\n",
    "for rnd graph on ppsface20, no weight 20percent connexions\n",
    "Mean clustering coef: 0.20056889237233466 Std clustering coef: 0.0019105323167833586\n",
    "Mean modularity: 0.06754303793904864 Std modularity: 0.0020174648024495236\n",
    "Mean average shortest path: 1.8002356419413383 Std average shortest path: 0.00157101733352845\n",
    "Mean small world: 0.9989269033708027 Std small world: 0.00326644634047313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Laplacian Eigenmodes\n",
    "\n",
    "From the correlation matrix we can derive the main eigenmodes, a bit like the Haak et al. paper.\n",
    "\n",
    "The correaltion matrix is transformed into a Laplacian graph.\n",
    "Negatives values are dealt like in the Haak et al. paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcel_correlation(group_parcel, \n",
    "                       surf_fmri):\n",
    "    \"\"\"\n",
    "    Group the fMRI data into parcels and compute the correlation between parcels.\n",
    "        group_parcel (np.ndarray): An array where each element indicates the parcel assignment \n",
    "                                   of the corresponding vertex in the surface fMRI data.\n",
    "        surf_fmri (np.ndarray): Single Subject fmri data, A 2D array where each row represents a vertex and each column \n",
    "                                represents a time point of the fMRI data.\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array representing the correlation matrix between the parcels.\n",
    "    \"\"\"\n",
    "    # Get unique parcel indices greater than 0 (non-zero parcels)\n",
    "    parcel_idx = np.unique(group_parcel[group_parcel >= 0])\n",
    "    n_parcels = len(parcel_idx)\n",
    "    \n",
    "    # Initialize an array to store mean fMRI data for each parcel\n",
    "    parcel_data = np.zeros((n_parcels, surf_fmri.shape[1]))\n",
    "    \n",
    "    # For each parcel, calculate the mean fMRI time series across vertices in the parcel\n",
    "    for i, idx in enumerate(parcel_idx):\n",
    "        # Boolean mask for vertices in the current parcel\n",
    "        mask = group_parcel == idx\n",
    "        # Compute the mean fMRI data for this parcel and store it\n",
    "        parcel_data[i] = np.mean(surf_fmri[mask], axis=0)\n",
    "        \n",
    "        # Check for NaN values in the computed parcel data\n",
    "        if np.isnan(parcel_data[i]).any():\n",
    "            print(f'Parcel {idx} has NaN values')\n",
    "    \n",
    "    # Compute and return the correlation matrix between parcels\n",
    "    parcel_corr = np.corrcoef(parcel_data)\n",
    "    return parcel_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "# from group_analysis import parcel_correlation\n",
    "\n",
    "# Make it for PPSFACE18 on run 1 and run 2\n",
    "ppsface = \"PPSFACE_N20\"\n",
    "# Load the parcellation for lh run1\n",
    "label_lh = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_lh.npy\")\n",
    "label_rh = np.load(f\"D:\\Data_Conn_Preproc\\{ppsface}\\group_parcellation\\labels_group_run12_smooth10_rh.npy\")\n",
    "label_rh[label_rh>=0] += np.max(label_lh) + 1 # 40k x 1, new values for the labels\n",
    "# Load all of the surf fmri data form run 1 lh\n",
    "surf_fmri_list_1_lh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'lh', run = '1')\n",
    "surf_fmri_list_2_lh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'lh', run = '2')\n",
    "surf_fmri_list_1_rh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'rh', run = '1')\n",
    "surf_fmri_list_2_rh = extract_fmri_timeseries(dataset = ppsface, hemisphere = 'rh', run = '2')\n",
    "\n",
    "\n",
    "# Get the parcellation correlatino for lh and rh\n",
    "subject = 0\n",
    "# Group de run 1 and 2\n",
    "run1_lh = surf_fmri_list_1_lh[subject]\n",
    "run2_lh = surf_fmri_list_2_lh[subject]\n",
    "run12_lh = np.concatenate((run1_lh, run2_lh), axis=1) # 40k x 760\n",
    "run1_rh = surf_fmri_list_1_rh[subject]\n",
    "run2_rh = surf_fmri_list_2_rh[subject]\n",
    "run12_rh = np.concatenate((run1_rh, run2_rh), axis=1) # 40k x 760\n",
    "label_both_hemi = np.concatenate((label_lh, label_rh), axis=0) # 80k x 1\n",
    "run12 = np.concatenate((run12_lh, run12_rh), axis=0) # 80k x 760\n",
    "parcel_corr = parcel_correlation(label_both_hemi, run12)\n",
    "\n",
    "# Get the 20% percentile of most correlate values\n",
    "min_corr = np.percentile(parcel_corr, 85)\n",
    "parcel_corr[parcel_corr<min_corr] = 0\n",
    "\n",
    "\n",
    "# Get the eigenmodes with a graph laplacian\n",
    "# W = parcel_corr + 1 # to have values between 0 and 2 (or keep only the strongest connections)\n",
    "W = parcel_corr\n",
    "D = np.diag(np.sum(W,0))\n",
    "L = np.subtract(D,W)\n",
    "# Compute the eigenvectors and eigenvalues\n",
    "eigvals, eigvecs = np.linalg.eig(L)\n",
    "idx = eigvals.argsort()\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:,idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eigenmaps with a PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "eigvecs_pca = pca.fit_transform(eigvecs)\n",
    "eigvecs_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fielder_lh = np.zeros_like(label_lh, dtype=float)\n",
    "fielder_lh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fielder vector is a 465 x 1 vector, i want to attribute the value of the parcels to the vertices\n",
    "\n",
    "# Get the fielder vector\n",
    "fielder = eigvecs[:,12] \n",
    "\n",
    "fielder_lh = np.zeros_like(label_lh, dtype=float)\n",
    "\n",
    "label_values = np.unique(label_lh[label_lh>= 0])\n",
    "\n",
    "for i in label_values:\n",
    "    # fielder_lh[label_lh == i] = fielder[i]\n",
    "    # fielder_lh[label_lh == i] = parcel_corr[0,i]\n",
    "    fielder_lh[label_lh == i] = eigvecs_pca[i,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = \"lh\"\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(inf_path)\n",
    "visualize_brain_surface(coords_lh, faces_lh, fielder_lh, title='Fielder Vector')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
