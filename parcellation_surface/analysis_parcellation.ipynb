{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal # Requires Python 3.8+\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from visualization import visualize_brain_surface\n",
    "from smoothing import smooth_surface_graph\n",
    "from watershed import watershed_by_flooding\n",
    "from gradient import build_mesh_graph\n",
    "\n",
    "from group_analysis import create_random_parcels, homogeneity_craddock_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fmri_timeseries(hemisphere: Literal[\"lh\", \"rh\"],\n",
    "                            run = Literal[\"1\",\"2\"]):\n",
    "    # Load all of the surf fmri data\n",
    "    surf_fmri_list = []\n",
    "    dataset_dir = r\"D:\\Data_Conn_Preproc\\PPSFACE_N18\"\n",
    "    for i in range(1,19):\n",
    "        # if i == 5: # Subject 5 is missing\n",
    "        #     continue\n",
    "        subject = f\"{i:02d}\"\n",
    "        subj_dir = dataset_dir + r\"\\sub-\" + subject\n",
    "        fmri_path = subj_dir + f\"\\\\func\\surf_conn_sub{subject}_run{run}_{hemisphere}.func.fsaverage6.mgh\"\n",
    "        surf_fmri_img = nib.load(fmri_path)\n",
    "        surf_fmri = surf_fmri_img.get_fdata()\n",
    "        surf_fmri = np.squeeze(surf_fmri) # just rearrange the MGH data\n",
    "        # Normilize the data\n",
    "        surf_fmri = (surf_fmri - np.mean(surf_fmri, axis=1, keepdims=True)) / np.std(surf_fmri, axis=1, keepdims=True)\n",
    "        # Replace nan values with 0\n",
    "        surf_fmri = np.nan_to_num(surf_fmri)\n",
    "        surf_fmri_list.append(surf_fmri)\n",
    "    return surf_fmri_list\n",
    "\n",
    "def extract_parcellation(hemi: Literal[\"lh\", \"rh\"], run = Literal[\"1\",\"2\"]):\n",
    "    # Load the parcellation\n",
    "    bound_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N20\\\\boundary_smoothed_map_allsub_run{run}_{hemi}.npy\"\n",
    "    grad_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N20\\gradients_smoothed_allsub_run{run}_{hemi}.npy\"\n",
    "    # bound_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\group_parcellation\\\\boundary_map_group_run{run}_{hemi}.npy\"\n",
    "    # grad_path = f\"D:\\Data_Conn_Preproc\\PPSFACE_N18\\group_parcellation\\gradients_smoothed_group_run{run}_{hemi}.npy\"\n",
    "    bound = np.load(bound_path)\n",
    "    grad = np.load(grad_path)\n",
    "    return bound, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show both hemisphere boundary maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import combine_surfaces\n",
    "# Load the parcellation for lh run1\n",
    "run = \"1\"\n",
    "\n",
    "inf_lh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\lh.white\"\n",
    "inf_rh_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\\\rh.white\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords_lh, faces_lh = nib.freesurfer.read_geometry(inf_lh_path)\n",
    "coords_rh, faces_rh = nib.freesurfer.read_geometry(inf_rh_path)\n",
    "\n",
    "# Extract bounardy and grad maps\n",
    "bound_lh, _ = extract_parcellation('lh', run)\n",
    "bound_rh, _ = extract_parcellation('rh', run)\n",
    "\n",
    "# Combine surface meshes\n",
    "coords_c, faces_c, scalar_c = combine_surfaces(coords_lh, faces_lh, bound_lh,\n",
    "                                              coords_rh, faces_rh, bound_rh)\n",
    "\n",
    "visualize_brain_surface(coords_c, faces_c, scalar_c, title='Boundary map ', cmap='cold_hot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Coef comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice coeff run 1 vs run 2 for the boundary maps\n",
    "from group_analysis import dice_coefficient\n",
    "hemi = \"lh\"\n",
    "run = \"1\"\n",
    "\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "graph = build_mesh_graph(faces)\n",
    "\n",
    "\n",
    "bound, grad = extract_parcellation(hemi, run)\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "label_bound_run1 = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "\n",
    "run = \"2\"\n",
    "bound, grad = extract_parcellation(hemi, run)\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "label_bound_run2 = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "dice_p = dice_coefficient((label_bound_run1>=0)*1, (label_bound_run2>=0)*1)\n",
    "dice_b = dice_coefficient((label_bound_run1<0)*1, (label_bound_run2<0)*1)\n",
    "\n",
    "print(f\"Number of parcels run1: {(np.unique(label_bound_run1)>=0).sum()}\")\n",
    "print(f\"Number of parcels run2: {(np.unique(label_bound_run2)>=0).sum()}\")\n",
    "\n",
    "\n",
    "print(f\"Run 1-2 Dice coefficient on parcels: {dice_p}\")\n",
    "print(f\"Run 1-2 Dice coefficient on boundaries: {dice_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_rnd = []\n",
    "dice_rnd_parc = []\n",
    "for i in range(100):\n",
    "    # Compare the Dice indice for random parcels of same size\n",
    "    rnd_parc_1 = create_random_parcels(graph,n_clusters=(np.unique(label_bound_run1)>=0).sum())\n",
    "    rnd_parc_2 = create_random_parcels(graph,n_clusters=(np.unique(label_bound_run2)>=0).sum())\n",
    "\n",
    "    dice_p_ = dice_coefficient((rnd_parc_1>=0)*1, (rnd_parc_2>=0)*1)\n",
    "    # dice_b_ = dice_coefficient((rnd_parc_1<0)*1, (rnd_parc_2<0)*1)\n",
    "    dice_rnd_parc.append(dice_p_)\n",
    "    # dice_rnd.append(dice_b_)\n",
    "    \n",
    "    # print(f\"Random 1-2 Dice coefficient on parcels: {dice_b_}\")\n",
    "    print(f\"Random 1-2 Dice coefficient on parcels: {dice_p_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the homogenity scores\n",
    "x_null = np.ones(len(dice_rnd_parc)) + np.random.uniform(-0.05, 0.05, len(dice_rnd_parc))\n",
    "x_model = np.array([1]) \n",
    "\n",
    "plt.figure(figsize=(2, 6))\n",
    "plt.scatter(x_model, [dice_p], color='red', marker='D', s=100, label='Proposed Model')\n",
    "plt.scatter(x_null, dice_rnd_parc, color='blue', alpha=0.7, label='Null Model')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([1],'')  # Single x-tick since both are on the same column\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.title('Consistency Between Run 1 and Run 2, LH')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an homogenity metrix like craddock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parcellation for lh run1\n",
    "hemi = \"lh\"\n",
    "run = \"1\"\n",
    "surface_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.white\"\n",
    "inf_path = f\"D:\\Data_Conn_Preproc\\\\fsaverage6\\surf\\{hemi}.inflated\"\n",
    "# surface_path = r\"D:\\Data_Conn_Preproc\\fsaverage6\\surf\\rh.inflated\"\n",
    "coords, faces = nib.freesurfer.read_geometry(surface_path)\n",
    "coords_, faces_ = nib.freesurfer.read_geometry(inf_path)\n",
    "graph = build_mesh_graph(faces)\n",
    "\n",
    "# Extract bounardy and grad maps\n",
    "bound, grad = extract_parcellation(hemi, run)\n",
    "\n",
    "bound_smoothed = smooth_surface_graph(graph, bound, iterations=10)\n",
    "grad_smoothed = smooth_surface_graph(graph, np.mean(grad,axis=1), iterations=10)\n",
    "\n",
    "label_grad = watershed_by_flooding(graph, grad_smoothed)\n",
    "label_bound = watershed_by_flooding(graph, bound_smoothed)\n",
    "\n",
    "# Load all of the surf fmri data form run 1 lh\n",
    "surf_fmri_list = extract_fmri_timeseries(hemi, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean and var', surf_fmri_list[2].mean(), surf_fmri_list[2].var())\n",
    "print('list length', len(surf_fmri_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_brain_surface(coords_, faces_, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_model = homogeneity_craddock_rt(label_bound, surf_fmri_list)\n",
    "print(f\"Model homogeneity: {homogeneity_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the null model on the\n",
    "homogenity_null_list = []\n",
    "for n_sample in range(1, 100):\n",
    "    label_null = create_random_parcels(graph, n_clusters=(np.unique(label_bound)>0).sum()) # Create a null model\n",
    "    homogeneity_null = homogeneity_craddock_rt(label_null, surf_fmri_list)\n",
    "    homogenity_null_list.append(homogeneity_null)\n",
    "    print(f\"Null homogeneity: {homogeneity_null}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the homogenity scores\n",
    "x_null = np.ones(len(homogenity_null_list)) + np.random.uniform(-0.05, 0.05, len(homogenity_null_list))\n",
    "x_model = np.array([1]) \n",
    "\n",
    "plt.figure(figsize=(2, 6))\n",
    "plt.scatter(x_model, [homogeneity_model], color='red', marker='D', s=100, label='Proposed Model')\n",
    "plt.scatter(x_null, homogenity_null_list, color='blue', alpha=0.7, label='Null Model')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([1],'')  # Single x-tick since both are on the same column\n",
    "plt.ylabel('Homogeneity Score (Craddock)')\n",
    "plt.title('Distribution of Null Scores vs. Single-Run Model Score')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
